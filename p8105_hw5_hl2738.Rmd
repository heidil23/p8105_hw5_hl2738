---
title: "P8105 Homework 5"
author: Heidi Lumish
output: github_document
---

```{r setup, include=FALSE}
library(readr)
library(tidyverse)
library(tidyr)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1: Homicides

#### Import the data
First we will import and clean the homicides dataset. We will replace blank cells and those marked "Unknown" with "NA." We will create a new city_state variable and a variable indicating whether the homicide was solved or unsolved, labeled "resolution." Finally, we will eliminate the "Tulsa, AL" location since this is clearly an error, as Tulsa is in OK.

```{r, message = FALSE}
urlfile="https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"

homicides_df = read_csv(url(urlfile), na = c("", "Unknown")) %>%
  mutate(city_state = paste(city, state, sep = ", "),
                     resolution = case_when(
                       disposition == "Closed without arrest" ~ "unsolved",
                       disposition == "Open/No arrest" ~ "unsolved",
                       disposition == "Closed by arrest" ~ "solved")) %>% 
  relocate(city_state) %>% 
  filter(city_state != "Tulsa, AL")
```

#### Baltimore, MD

Next, we will use prop.test to estimate the proportion of homicides that are unsolved.

```{r}
baltimore_df =
  homicides_df %>% 
  filter(city_state == "Baltimore, MD")

baltimore_summary = baltimore_df %>% 
  summarize(
    unsolved = sum(resolution == "unsolved"),
    n = n()
  )

baltimore_test = 
  prop.test(
  x = baltimore_summary %>% pull(unsolved),
  n = baltimore_summary %>% pull(n))

baltimore_test %>% 
  broom::tidy() %>% 
  knitr::kable(
    caption = "Estimate for proportion of unsolved homicides in Baltimore, MD"
  )
```

#### All cities
Next we will run prop.test across all cities. First we will write a function to summarize the data for each city and then run prop.test.

```{r}
prop_test_function = function(df) {
  
  city_summary = 
    df %>% 
    summarize(
      unsolved = sum(resolution == "unsolved"),
      n = n()
    )
  
  city_test = 
    prop.test(
      x = city_summary %>% pull(unsolved),
      n = city_summary %>% pull(n))
  
  return(city_test)
  
}
```

Next we will use the map function to iterate across cities.

```{r}
results_df = homicides_df %>% 
  nest(data = uid:resolution) %>% 
  mutate(
    test_results = map(data, prop_test_function),
    tidy_results = map(test_results, broom::tidy)
  ) %>% 
  select(city_state, tidy_results) %>% 
  unnest(tidy_results) %>% 
  select(city_state, estimate, starts_with("conf"))

results_df %>% knitr::kable()
```

Finally, we will create a plot showing the estimates and confidence intervals for each city.

```{r}
results_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) + 
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  labs(
    title = "Estimated proportion of unsolved homicides by city",
    x = "City",
    y = "Estimate")
```

## Problem 2: Longitudinal Study

#### Import and tidy the data

First we will import the data from the longitudinal study. Since data for each participant is included in a separate file, first we create a dataframe containing all file names (which are the subject ID and arm). Next we will write a function to read data from the csv files. Then we will use the purrr::map function to read in the data for each subject. Finally, we will tidy the data by unnesting it, separating the arm and subject IDs into separate columns, and using pivot longer to create a "week" variable.

```{r, message = FALSE}
files_df = 
  tibble(file_name = list.files("./data")) 

subject_data = function(file_name){
  
  data = read_csv(str_c("./data/", file_name))
  
  return(data)
  
}

observations = files_df %>% 
  mutate(data = map(file_name, subject_data)) %>%
  unnest(data) %>%
  separate(file_name, c("arm", "subject_id"), "_") %>%
  mutate(subject_id = substr(subject_id, 1, 2),
         subject_id = as.numeric(subject_id)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    names_prefix = "week_",
    values_to = "observations"
  ) %>% 
  mutate(week = as.numeric(week))
```

#### Spaghetti plot

Next we will make a spaghetti plot showing observations on each subject over time. 

```{r}
observations %>% 
  ggplot(aes(x = week, y = observations, color = arm)) +
  geom_point(size = .2) + 
  geom_line(aes(group = interaction(arm, subject_id)), alpha = .3) +
  labs(
    title = "Observations among experimental and control groups over time",
    x = "Week",
    y = "Observed Values") +
  scale_color_discrete(name = "Arm", labels = c("Control", "Experimental"))
```

The spaghetti plot shows that the values among the control subjects are relatively constant over time, whereas the values for the experimental subjects increase over time.

## Problem 3: Iris dataset

First we will load the iris dataset and introduce missing values, using the sample code.

```{r}
set.seed(10)

iris_with_missing = iris %>% 
  map_df(~replace(.x, sample(1:150, 20), NA)) %>%
  mutate(Species = as.character(Species))
```

Next, we will write a function that replaces missing values. For numeric variables, the missing values will be filled in with the mean of the non-missing variables. For character variables, missing values will be filled in with "virginica."

```{r}
missing = function(x){
  if (is.character(x)) {
    x[is.na(x)] <- "virginica"
  }
  
  if (is.numeric(x)) {

  x[is.na(x)] <- mean(x, na.rm = TRUE)

  }
  
  return(x)
}
```

Next we will apply the "missing" function written above to the iris_with_missing dataset and create a new dataset named iris_no_missing. By counting "NA" values, we see that the 100 missing values have now been filled in.

```{r}
iris_no_missing = iris_with_missing %>%
  map_df(missing)

sum(is.na(iris_with_missing))

sum(is.na(iris_no_missing))
```